2025-12-22 21:13:34,586 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-22 21:13:36,653 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-22 21:16:44,997 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-22 21:17:37,546 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2025-12-22 21:17:37,583 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-22 21:17:37,584 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-12-22 21:17:37,587 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-22 21:17:37,589 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-22 21:18:26,821 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-22 21:21:44,750 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2025-12-22 21:21:44,752 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-22 21:21:44,753 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-12-22 21:21:44,756 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-22 21:21:44,757 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-22 21:22:03,822 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-22 21:26:35,500 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2025-12-22 21:26:35,505 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-22 21:26:35,506 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-12-22 21:26:35,514 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-22 21:26:35,515 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-22 22:23:32,118 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-22 22:23:35,153 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-22 22:24:13,944 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-22 22:24:18,111 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-22 22:28:13,093 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-22 22:39:44,838 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2025-12-22 22:39:44,899 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-22 22:39:44,900 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-12-22 22:39:44,903 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-22 22:39:44,904 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-24 11:54:10,976 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-24 11:54:17,732 | INFO     | py4j.java_gateway | Callback Server Starting
2025-12-24 11:54:17,733 | INFO     | py4j.java_gateway | Socket listening on ('127.0.0.1', 64090)
2025-12-24 11:54:20,902 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-24 11:59:28,368 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-24 11:59:34,406 | INFO     | py4j.java_gateway | Callback Server Starting
2025-12-24 11:59:34,407 | INFO     | py4j.java_gateway | Socket listening on ('127.0.0.1', 64296)
2025-12-24 11:59:34,979 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-24 12:04:50,773 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-24 12:04:57,243 | INFO     | py4j.java_gateway | Callback Server Starting
2025-12-24 12:04:57,244 | INFO     | py4j.java_gateway | Socket listening on ('127.0.0.1', 64526)
2025-12-24 12:05:00,919 | INFO     | py4j.clientserver | Python Server ready to receive messages
2025-12-24 12:05:00,923 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-24 12:05:01,180 | ERROR    | py4j.clientserver | There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 120, in call
    raise e
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "D:\dev\UCM-BD_DE\votes_manager\streaming\jobs\votes_streaming_job.py", line 161, in _write_to_elasticsearch
    .save()
     ^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1461, in save
    self._jwrite.save()
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o98.save.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: org.elasticsearch.spark.sql. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:260)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy30.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: java.lang.ClassNotFoundException: org.elasticsearch.spark.sql.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:221)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 48 more

2025-12-24 12:07:24,977 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2025-12-24 12:07:24,979 | INFO     | py4j.clientserver | Error while python server was waiting fora message
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 566, in wait_for_commands
    command = smart_decode(self.stream.readline())[:-1]
                           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2025-12-24 12:07:25,016 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-24 12:07:25,017 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-24 12:07:25,017 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-12-24 12:07:25,018 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-24 12:07:25,020 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-24 12:23:19,404 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-24 12:23:27,467 | INFO     | py4j.java_gateway | Callback Server Starting
2025-12-24 12:23:27,468 | INFO     | py4j.java_gateway | Socket listening on ('127.0.0.1', 65062)
2025-12-24 12:23:30,738 | INFO     | py4j.clientserver | Python Server ready to receive messages
2025-12-24 12:23:30,739 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-24 12:23:31,022 | ERROR    | py4j.clientserver | There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 120, in call
    raise e
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "D:\dev\UCM-BD_DE\votes_manager\streaming\jobs\votes_streaming_job.py", line 166, in _write_to_elasticsearch
    .save()
     ^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1461, in save
    self._jwrite.save()
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o105.save.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: org.elasticsearch.spark.sql. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:260)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy30.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: java.lang.ClassNotFoundException: org.elasticsearch.spark.sql.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:221)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 48 more

2025-12-24 12:29:13,487 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2025-12-24 12:29:13,490 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-24 12:29:13,490 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-12-24 12:29:13,491 | INFO     | py4j.clientserver | Error while python server was waiting fora message
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 566, in wait_for_commands
    command = smart_decode(self.stream.readline())[:-1]
                           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2025-12-24 12:29:13,492 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-24 12:29:13,493 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-24 12:29:13,495 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-24 12:30:20,751 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-24 12:30:26,379 | INFO     | py4j.java_gateway | Callback Server Starting
2025-12-24 12:30:26,379 | INFO     | py4j.java_gateway | Socket listening on ('127.0.0.1', 65412)
2025-12-24 12:30:29,411 | INFO     | py4j.clientserver | Python Server ready to receive messages
2025-12-24 12:30:30,630 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-24 12:30:34,660 | ERROR    | py4j.clientserver | There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 120, in call
    raise e
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "D:\dev\UCM-BD_DE\votes_manager\streaming\jobs\votes_streaming_job.py", line 166, in _write_to_elasticsearch
    .save()
     ^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1461, in save
    self._jwrite.save()
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o105.save.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 1.0 failed 1 times, most recent failure: Lost task 7.0 in stage 1.0 (TID 7) (DESKTOP-HMVAKG6.mshome.net executor driver): org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Detected type name in resource [votes_by_party/_doc]. Remove type name to continue.
	at org.elasticsearch.hadoop.rest.Resource.<init>(Resource.java:88)
	at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:595)
	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:71)
	at org.elasticsearch.spark.sql.EsSparkSQL$.$anonfun$saveToEs$1(EsSparkSQL.scala:103)
	at org.elasticsearch.spark.sql.EsSparkSQL$.$anonfun$saveToEs$1$adapted(EsSparkSQL.scala:103)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2898)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2834)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2833)
	at scala.collection.immutable.List.foreach(List.scala:333)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2833)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1253)
	at scala.Option.foreach(Option.scala:437)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3102)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3036)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3025)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:995)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2446)
	at org.elasticsearch.spark.sql.EsSparkSQL$.saveToEs(EsSparkSQL.scala:103)
	at org.elasticsearch.spark.sql.ElasticsearchRelation.insert(DefaultSource.scala:629)
	at org.elasticsearch.spark.sql.DefaultSource.createRelation(DefaultSource.scala:107)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy31.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Detected type name in resource [votes_by_party/_doc]. Remove type name to continue.
	at org.elasticsearch.hadoop.rest.Resource.<init>(Resource.java:88)
	at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:595)
	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:71)
	at org.elasticsearch.spark.sql.EsSparkSQL$.$anonfun$saveToEs$1(EsSparkSQL.scala:103)
	at org.elasticsearch.spark.sql.EsSparkSQL$.$anonfun$saveToEs$1$adapted(EsSparkSQL.scala:103)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)

2025-12-24 12:50:03,113 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2025-12-24 12:50:03,123 | INFO     | py4j.clientserver | Error while python server was waiting fora message
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 566, in wait_for_commands
    command = smart_decode(self.stream.readline())[:-1]
                           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2025-12-24 12:50:03,125 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-24 12:50:03,125 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-24 12:50:03,126 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-12-24 12:50:03,128 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-24 12:50:03,128 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-24 12:50:37,243 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-24 12:50:43,135 | INFO     | py4j.java_gateway | Callback Server Starting
2025-12-24 12:50:43,135 | INFO     | py4j.java_gateway | Socket listening on ('127.0.0.1', 50261)
2025-12-24 12:50:46,547 | INFO     | py4j.clientserver | Python Server ready to receive messages
2025-12-24 12:50:46,547 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-24 12:51:38,217 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-24 12:52:17,785 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-24 13:02:19,346 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-24 13:03:00,159 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-24 13:03:35,697 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-24 13:04:14,911 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-24 13:04:56,241 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-24 13:05:33,944 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-24 13:16:50,338 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2025-12-24 13:16:50,339 | INFO     | py4j.clientserver | Error while python server was waiting fora message
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 566, in wait_for_commands
    command = smart_decode(self.stream.readline())[:-1]
                           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2025-12-24 13:16:50,352 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-24 13:16:50,354 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-24 13:16:50,354 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-12-24 13:16:50,359 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-24 13:16:50,360 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-30 08:48:24,182 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-30 08:48:30,821 | INFO     | py4j.java_gateway | Callback Server Starting
2025-12-30 08:48:30,822 | INFO     | py4j.java_gateway | Socket listening on ('127.0.0.1', 56863)
2025-12-30 08:48:32,965 | INFO     | py4j.clientserver | Python Server ready to receive messages
2025-12-30 08:48:32,966 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 08:50:43,694 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 08:50:50,316 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 08:50:52,715 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 08:50:54,502 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 08:50:56,230 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 08:50:58,038 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 08:51:00,094 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 08:51:03,022 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 09:01:18,659 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 09:01:20,051 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 09:01:21,610 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 09:01:22,892 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 09:01:24,064 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 09:01:25,311 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 09:01:26,644 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 09:01:28,198 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 09:01:29,682 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 09:01:31,039 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 09:03:14,816 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 09:03:16,268 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 09:03:17,549 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 09:03:18,924 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 09:03:20,099 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 09:03:21,388 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 09:03:22,692 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 09:03:23,881 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 09:03:25,097 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 09:03:26,258 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 09:03:28,383 | INFO     | py4j.clientserver | Received command c on object id p0
2025-12-30 09:34:11,677 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-31 09:22:47,774 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-31 09:22:56,498 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-31 09:24:40,856 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-31 09:26:55,797 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2025-12-31 09:26:55,831 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-31 09:26:55,832 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-12-31 09:26:55,836 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-31 09:26:55,837 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-31 09:30:08,230 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-31 09:30:11,791 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-31 09:30:41,465 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-31 09:37:48,545 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2025-12-31 09:37:48,548 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-31 09:37:48,549 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-12-31 09:37:48,552 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-31 09:37:48,554 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-31 13:14:36,834 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-31 13:14:44,009 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-31 14:36:10,841 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-31 14:37:07,788 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2025-12-31 14:37:07,795 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-31 14:37:07,796 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-12-31 14:37:07,800 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-31 14:37:07,801 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-31 15:23:11,237 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-31 15:23:15,244 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-31 15:30:11,020 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-31 15:30:14,817 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-31 15:32:22,478 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-31 15:32:26,012 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-31 15:34:58,471 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-31 15:43:49,039 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2025-12-31 15:43:49,047 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-31 15:43:49,047 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-12-31 15:43:49,051 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-31 15:43:49,052 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-31 16:12:31,017 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-31 16:12:35,333 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-31 16:14:14,401 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-31 16:14:18,603 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-31 16:19:18,505 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-31 16:19:22,382 | INFO     | py4j.clientserver | Closing down clientserver connection
2025-12-31 16:20:24,678 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2025-12-31 16:20:28,394 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-02 20:09:00,699 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-02 20:09:05,456 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-02 20:15:05,769 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-02 20:15:10,549 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-02 20:16:42,222 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-02 20:16:47,627 | INFO     | py4j.java_gateway | Callback Server Starting
2026-01-02 20:16:47,627 | INFO     | py4j.java_gateway | Socket listening on ('127.0.0.1', 55905)
2026-01-02 20:16:48,081 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-02 20:31:57,465 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-02 20:32:02,753 | INFO     | py4j.java_gateway | Callback Server Starting
2026-01-02 20:32:02,754 | INFO     | py4j.java_gateway | Socket listening on ('127.0.0.1', 56097)
2026-01-02 20:32:44,185 | INFO     | py4j.clientserver | Python Server ready to receive messages
2026-01-02 20:32:44,186 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-02 20:35:26,550 | ERROR    | py4j.clientserver | There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 120, in call
    raise e
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "D:\dev\UCM-BD_DE\votes_manager\streaming\jobs\votes_streaming_job.py", line 301, in write_gold_all
    .save()
     ^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1461, in save
    self._jwrite.save()
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o136.save.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 10.0 failed 1 times, most recent failure: Lost task 7.0 in stage 10.0 (TID 613) (DESKTOP-HMVAKG6.mshome.net executor driver): org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[172.18.0.3:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:160)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:442)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:438)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:398)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:402)
	at org.elasticsearch.hadoop.rest.RestClient.get(RestClient.java:178)
	at org.elasticsearch.hadoop.rest.request.GetAliasesRequestBuilder.execute(GetAliasesRequestBuilder.java:68)
	at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:620)
	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:71)
	at org.elasticsearch.spark.sql.EsSparkSQL$.$anonfun$saveToEs$1(EsSparkSQL.scala:103)
	at org.elasticsearch.spark.sql.EsSparkSQL$.$anonfun$saveToEs$1$adapted(EsSparkSQL.scala:103)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2898)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2834)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2833)
	at scala.collection.immutable.List.foreach(List.scala:333)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2833)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1253)
	at scala.Option.foreach(Option.scala:437)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3102)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3036)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3025)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:995)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2446)
	at org.elasticsearch.spark.sql.EsSparkSQL$.saveToEs(EsSparkSQL.scala:103)
	at org.elasticsearch.spark.sql.ElasticsearchRelation.insert(DefaultSource.scala:629)
	at org.elasticsearch.spark.sql.DefaultSource.createRelation(DefaultSource.scala:108)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy32.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[172.18.0.3:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:160)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:442)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:438)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:398)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:402)
	at org.elasticsearch.hadoop.rest.RestClient.get(RestClient.java:178)
	at org.elasticsearch.hadoop.rest.request.GetAliasesRequestBuilder.execute(GetAliasesRequestBuilder.java:68)
	at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:620)
	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:71)
	at org.elasticsearch.spark.sql.EsSparkSQL$.$anonfun$saveToEs$1(EsSparkSQL.scala:103)
	at org.elasticsearch.spark.sql.EsSparkSQL$.$anonfun$saveToEs$1$adapted(EsSparkSQL.scala:103)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)

2026-01-02 20:39:28,635 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-02 20:39:28,644 | INFO     | py4j.clientserver | Error while python server was waiting fora message
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 566, in wait_for_commands
    command = smart_decode(self.stream.readline())[:-1]
                           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-02 20:39:28,661 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-02 20:39:28,661 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-02 20:39:28,661 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-01-02 20:39:28,677 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-02 20:39:28,677 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-02 20:39:52,620 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-02 20:39:58,864 | INFO     | py4j.java_gateway | Callback Server Starting
2026-01-02 20:39:58,864 | INFO     | py4j.java_gateway | Socket listening on ('127.0.0.1', 56413)
2026-01-02 20:40:44,701 | INFO     | py4j.clientserver | Python Server ready to receive messages
2026-01-02 20:40:44,702 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-02 20:41:34,542 | ERROR    | py4j.clientserver | There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 120, in call
    raise e
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "D:\dev\UCM-BD_DE\votes_manager\streaming\jobs\votes_streaming_job.py", line 301, in write_gold_all
    .save()
     ^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1461, in save
    self._jwrite.save()
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o136.save.
: org.apache.spark.SparkException: Job 6 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1259)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1257)
	at scala.collection.mutable.HashSet$Node.foreach(HashSet.scala:435)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:361)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1257)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3129)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:3015)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:3015)
	at org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2258)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2258)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2211)
	at org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:681)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:995)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2446)
	at org.elasticsearch.spark.sql.EsSparkSQL$.saveToEs(EsSparkSQL.scala:103)
	at org.elasticsearch.spark.sql.ElasticsearchRelation.insert(DefaultSource.scala:629)
	at org.elasticsearch.spark.sql.DefaultSource.createRelation(DefaultSource.scala:108)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy34.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)

2026-01-02 20:41:35,490 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-02 20:41:35,492 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-02 20:41:35,492 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-01-02 20:41:35,493 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-02 20:41:35,494 | INFO     | py4j.clientserver | Error while python server was waiting fora message
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 566, in wait_for_commands
    command = smart_decode(self.stream.readline())[:-1]
                           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-03 12:41:26,563 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-03 12:42:27,779 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 12:43:36,862 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-03 12:49:41,203 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 14:15:54,374 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-03 14:16:01,589 | INFO     | py4j.java_gateway | Callback Server Starting
2026-01-03 14:16:01,589 | INFO     | py4j.java_gateway | Socket listening on ('127.0.0.1', 63652)
2026-01-03 14:16:48,342 | INFO     | py4j.clientserver | Python Server ready to receive messages
2026-01-03 14:16:48,342 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-03 14:17:31,084 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 0 - Count: 0
2026-01-03 14:20:09,845 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-03 14:20:47,640 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 1 - Count: 216
2026-01-03 14:23:18,475 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-03 14:24:06,237 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 2 - Count: 276
2026-01-03 14:27:32,309 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-03 14:28:24,758 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 3 - Count: 307
2026-01-03 14:32:45,838 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-03 14:33:44,217 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 4 - Count: 382
2026-01-03 14:35:12,789 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-03 14:36:08,286 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 5 - Count: 382
2026-01-03 14:43:06,138 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-03 14:43:06,142 | INFO     | py4j.clientserver | Error while python server was waiting fora message
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 566, in wait_for_commands
    command = smart_decode(self.stream.readline())[:-1]
                           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-03 14:43:06,200 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 14:43:06,201 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 14:43:06,203 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-01-03 14:43:08,274 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 14:43:08,278 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 19:16:24,776 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-03 19:18:10,017 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-03 19:18:10,061 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 19:18:10,062 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-01-03 19:18:10,065 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 19:18:10,066 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 19:20:47,912 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-03 19:20:49,729 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 22:20:57,000 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-03 22:21:02,023 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 22:22:34,052 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-03 22:25:41,748 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 22:38:06,382 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-03 22:38:33,491 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-03 22:38:33,493 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 22:38:33,493 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-01-03 22:38:33,493 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 22:38:33,493 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 22:44:25,384 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-03 22:44:32,907 | INFO     | py4j.java_gateway | Callback Server Starting
2026-01-03 22:44:32,908 | INFO     | py4j.java_gateway | Socket listening on ('127.0.0.1', 49499)
2026-01-03 22:44:32,967 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 22:49:19,356 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-03 22:49:28,040 | INFO     | py4j.java_gateway | Callback Server Starting
2026-01-03 22:49:28,040 | INFO     | py4j.java_gateway | Socket listening on ('127.0.0.1', 49653)
2026-01-03 22:50:11,093 | INFO     | py4j.clientserver | Python Server ready to receive messages
2026-01-03 22:50:11,094 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-03 22:50:55,370 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-03 22:51:46,029 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-03 22:51:49,229 | ERROR    | py4j.clientserver | There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 120, in call
    raise e
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "D:\dev\UCM-BD_DE\votes_manager\streaming\jobs\votes_streaming_job.py", line 229, in _write_to_elasticsearch
    .save()
     ^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1461, in save
    self._jwrite.save()
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o161.save.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 14.0 failed 1 times, most recent failure: Lost task 2.0 in stage 14.0 (TID 802) (DESKTOP-HMVAKG6.mshome.net executor driver): org.apache.spark.util.TaskCompletionListenerException: Could not write all entries for bulk operation [1/74]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: version_conflict_engine_exception: [971356e74a7b37303e82d641c2c90069bd4edcace93fe925efe7c5d1401d7a4e]: version conflict, required seqNo [152], primary term [1]. current document has seqNo [204] and primary term [1]
	{"update":{"_id":"971356e74a7b37303e82d641c2c90069bd4edcace93fe925efe7c5d1401d7a4e"}}
{"doc_as_upsert":true,"doc":{"id":"4acb0e69-b684-4ed0-b448-cda4063f4286","blank_vote":false,"political_party":"Lechuga Verde","province_name":"Madrid","province_iso_code":"ES-M","autonomic_region_name":"Comunidad de Madrid","autonomic_region_iso_code":"ES-MD","location":"40.416800,-3.703800","timestamp":1767473412676,"doc_id":"971356e74a7b37303e82d641c2c90069bd4edcace93fe925efe7c5d1401d7a4e"}}

Bailing out...
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:254)
	at org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:185)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
	Suppressed: org.elasticsearch.hadoop.EsHadoopException: Could not write all entries for bulk operation [1/74]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: version_conflict_engine_exception: [971356e74a7b37303e82d641c2c90069bd4edcace93fe925efe7c5d1401d7a4e]: version conflict, required seqNo [152], primary term [1]. current document has seqNo [204] and primary term [1]
	{"update":{"_id":"971356e74a7b37303e82d641c2c90069bd4edcace93fe925efe7c5d1401d7a4e"}}
{"doc_as_upsert":true,"doc":{"id":"4acb0e69-b684-4ed0-b448-cda4063f4286","blank_vote":false,"political_party":"Lechuga Verde","province_name":"Madrid","province_iso_code":"ES-M","autonomic_region_name":"Comunidad de Madrid","autonomic_region_iso_code":"ES-MD","location":"40.416800,-3.703800","timestamp":1767473412676,"doc_id":"971356e74a7b37303e82d641c2c90069bd4edcace93fe925efe7c5d1401d7a4e"}}

Bailing out...
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.flush(BulkProcessor.java:538)
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.close(BulkProcessor.java:560)
		at org.elasticsearch.hadoop.rest.RestRepository.close(RestRepository.java:226)
		at org.elasticsearch.hadoop.rest.RestService$PartitionWriter.close(RestService.java:122)
		at org.elasticsearch.spark.rdd.EsRDDWriter$$anon$1.onTaskCompletion(EsRDDWriter.scala:74)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)
		... 12 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2898)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2834)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2833)
	at scala.collection.immutable.List.foreach(List.scala:333)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2833)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1253)
	at scala.Option.foreach(Option.scala:437)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3102)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3036)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3025)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:995)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2446)
	at org.elasticsearch.spark.sql.EsSparkSQL$.saveToEs(EsSparkSQL.scala:103)
	at org.elasticsearch.spark.sql.ElasticsearchRelation.insert(DefaultSource.scala:629)
	at org.elasticsearch.spark.sql.DefaultSource.createRelation(DefaultSource.scala:107)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy34.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.spark.util.TaskCompletionListenerException: Could not write all entries for bulk operation [1/74]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: version_conflict_engine_exception: [971356e74a7b37303e82d641c2c90069bd4edcace93fe925efe7c5d1401d7a4e]: version conflict, required seqNo [152], primary term [1]. current document has seqNo [204] and primary term [1]
	{"update":{"_id":"971356e74a7b37303e82d641c2c90069bd4edcace93fe925efe7c5d1401d7a4e"}}
{"doc_as_upsert":true,"doc":{"id":"4acb0e69-b684-4ed0-b448-cda4063f4286","blank_vote":false,"political_party":"Lechuga Verde","province_name":"Madrid","province_iso_code":"ES-M","autonomic_region_name":"Comunidad de Madrid","autonomic_region_iso_code":"ES-MD","location":"40.416800,-3.703800","timestamp":1767473412676,"doc_id":"971356e74a7b37303e82d641c2c90069bd4edcace93fe925efe7c5d1401d7a4e"}}

Bailing out...
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:254)
	at org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:185)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
	Suppressed: org.elasticsearch.hadoop.EsHadoopException: Could not write all entries for bulk operation [1/74]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: version_conflict_engine_exception: [971356e74a7b37303e82d641c2c90069bd4edcace93fe925efe7c5d1401d7a4e]: version conflict, required seqNo [152], primary term [1]. current document has seqNo [204] and primary term [1]
	{"update":{"_id":"971356e74a7b37303e82d641c2c90069bd4edcace93fe925efe7c5d1401d7a4e"}}
{"doc_as_upsert":true,"doc":{"id":"4acb0e69-b684-4ed0-b448-cda4063f4286","blank_vote":false,"political_party":"Lechuga Verde","province_name":"Madrid","province_iso_code":"ES-M","autonomic_region_name":"Comunidad de Madrid","autonomic_region_iso_code":"ES-MD","location":"40.416800,-3.703800","timestamp":1767473412676,"doc_id":"971356e74a7b37303e82d641c2c90069bd4edcace93fe925efe7c5d1401d7a4e"}}

Bailing out...
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.flush(BulkProcessor.java:538)
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.close(BulkProcessor.java:560)
		at org.elasticsearch.hadoop.rest.RestRepository.close(RestRepository.java:226)
		at org.elasticsearch.hadoop.rest.RestService$PartitionWriter.close(RestService.java:122)
		at org.elasticsearch.spark.rdd.EsRDDWriter$$anon$1.onTaskCompletion(EsRDDWriter.scala:74)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)
		... 12 more

2026-01-03 22:52:21,097 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-03 22:52:21,101 | INFO     | py4j.clientserver | Error while python server was waiting fora message
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 566, in wait_for_commands
    command = smart_decode(self.stream.readline())[:-1]
                           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-03 22:52:21,147 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 22:52:21,147 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 22:52:21,148 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-01-03 22:52:21,149 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 22:52:21,150 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 22:52:56,404 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-03 22:53:01,883 | INFO     | py4j.java_gateway | Callback Server Starting
2026-01-03 22:53:01,884 | INFO     | py4j.java_gateway | Socket listening on ('127.0.0.1', 49854)
2026-01-03 22:53:04,366 | INFO     | py4j.clientserver | Python Server ready to receive messages
2026-01-03 22:53:04,366 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-03 22:53:04,430 | ERROR    | py4j.clientserver | There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 120, in call
    raise e
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "D:\dev\UCM-BD_DE\votes_manager\streaming\jobs\votes_streaming_job.py", line 224, in _write_to_elasticsearch
    .mode("complete")
     ^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1029, in mode
    self._jwrite = self._jwrite.mode(saveMode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\errors\exceptions\captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Unknown save mode: complete. Accepted save modes are 'overwrite', 'append', 'ignore', 'error', 'errorifexists', 'default'.
2026-01-03 22:53:40,393 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-03 22:53:40,394 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 22:53:40,394 | INFO     | py4j.clientserver | Error while python server was waiting fora message
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 566, in wait_for_commands
    command = smart_decode(self.stream.readline())[:-1]
                           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-03 22:53:40,395 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-01-03 22:53:40,397 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 22:53:40,398 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 22:53:40,399 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 22:54:52,771 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-03 22:54:59,352 | INFO     | py4j.java_gateway | Callback Server Starting
2026-01-03 22:54:59,353 | INFO     | py4j.java_gateway | Socket listening on ('127.0.0.1', 49981)
2026-01-03 22:55:41,510 | INFO     | py4j.clientserver | Python Server ready to receive messages
2026-01-03 22:55:41,513 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-03 22:55:42,194 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-03 22:55:42,198 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 22:55:42,199 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-03 22:55:42,201 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-01-03 22:55:42,202 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 22:55:42,205 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 22:55:42,205 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-01-03 22:56:38,877 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-03 22:56:44,853 | INFO     | py4j.java_gateway | Callback Server Starting
2026-01-03 22:56:44,854 | INFO     | py4j.java_gateway | Socket listening on ('127.0.0.1', 50177)
2026-01-03 22:57:22,748 | INFO     | py4j.clientserver | Python Server ready to receive messages
2026-01-03 22:57:22,749 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-03 22:57:23,180 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 0 - Count: 0
2026-01-03 23:00:41,220 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-03 23:00:41,493 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 1 - Count: 3
2026-01-03 23:01:17,843 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-03 23:01:22,773 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 2 - Count: 590
2026-01-03 23:02:00,632 | ERROR    | py4j.clientserver | There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 120, in call
    raise e
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "D:\dev\UCM-BD_DE\votes_manager\streaming\jobs\votes_streaming_job.py", line 231, in _write_to_elasticsearch
    .save()
     ^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1461, in save
    self._jwrite.save()
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o161.save.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 22.0 failed 1 times, most recent failure: Lost task 0.0 in stage 22.0 (TID 1019) (DESKTOP-HMVAKG6.mshome.net executor driver): org.apache.spark.util.TaskCompletionListenerException: Could not write all entries for bulk operation [1/129]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: version_conflict_engine_exception: [9a795f736282d6baf6389d3a303008f5f31e1c163635623c57b677644af3dbb0]: version conflict, required seqNo [638], primary term [1]. current document has seqNo [733] and primary term [1]
	{"update":{"_id":"9a795f736282d6baf6389d3a303008f5f31e1c163635623c57b677644af3dbb0"}}
{"doc_as_upsert":true,"doc":{"id":"0ec9dade-c0bb-4b47-bab2-c258ed304a30","blank_vote":false,"political_party":"Gato Unido","province_name":"Málaga","province_iso_code":"ES-MA","autonomic_region_name":"Andalucía","autonomic_region_iso_code":"ES-AN","location":"36.721300,-4.421400","timestamp":1767474008767,"doc_id":"9a795f736282d6baf6389d3a303008f5f31e1c163635623c57b677644af3dbb0"}}

Bailing out...
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:254)
	at org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:185)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
	Suppressed: org.elasticsearch.hadoop.EsHadoopException: Could not write all entries for bulk operation [1/129]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: version_conflict_engine_exception: [9a795f736282d6baf6389d3a303008f5f31e1c163635623c57b677644af3dbb0]: version conflict, required seqNo [638], primary term [1]. current document has seqNo [733] and primary term [1]
	{"update":{"_id":"9a795f736282d6baf6389d3a303008f5f31e1c163635623c57b677644af3dbb0"}}
{"doc_as_upsert":true,"doc":{"id":"0ec9dade-c0bb-4b47-bab2-c258ed304a30","blank_vote":false,"political_party":"Gato Unido","province_name":"Málaga","province_iso_code":"ES-MA","autonomic_region_name":"Andalucía","autonomic_region_iso_code":"ES-AN","location":"36.721300,-4.421400","timestamp":1767474008767,"doc_id":"9a795f736282d6baf6389d3a303008f5f31e1c163635623c57b677644af3dbb0"}}

Bailing out...
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.flush(BulkProcessor.java:538)
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.close(BulkProcessor.java:560)
		at org.elasticsearch.hadoop.rest.RestRepository.close(RestRepository.java:226)
		at org.elasticsearch.hadoop.rest.RestService$PartitionWriter.close(RestService.java:122)
		at org.elasticsearch.spark.rdd.EsRDDWriter$$anon$1.onTaskCompletion(EsRDDWriter.scala:74)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)
		... 12 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2898)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2834)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2833)
	at scala.collection.immutable.List.foreach(List.scala:333)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2833)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1253)
	at scala.Option.foreach(Option.scala:437)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3102)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3036)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3025)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:995)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2446)
	at org.elasticsearch.spark.sql.EsSparkSQL$.saveToEs(EsSparkSQL.scala:103)
	at org.elasticsearch.spark.sql.ElasticsearchRelation.insert(DefaultSource.scala:629)
	at org.elasticsearch.spark.sql.DefaultSource.createRelation(DefaultSource.scala:107)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy34.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.spark.util.TaskCompletionListenerException: Could not write all entries for bulk operation [1/129]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: version_conflict_engine_exception: [9a795f736282d6baf6389d3a303008f5f31e1c163635623c57b677644af3dbb0]: version conflict, required seqNo [638], primary term [1]. current document has seqNo [733] and primary term [1]
	{"update":{"_id":"9a795f736282d6baf6389d3a303008f5f31e1c163635623c57b677644af3dbb0"}}
{"doc_as_upsert":true,"doc":{"id":"0ec9dade-c0bb-4b47-bab2-c258ed304a30","blank_vote":false,"political_party":"Gato Unido","province_name":"Málaga","province_iso_code":"ES-MA","autonomic_region_name":"Andalucía","autonomic_region_iso_code":"ES-AN","location":"36.721300,-4.421400","timestamp":1767474008767,"doc_id":"9a795f736282d6baf6389d3a303008f5f31e1c163635623c57b677644af3dbb0"}}

Bailing out...
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:254)
	at org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:185)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
	Suppressed: org.elasticsearch.hadoop.EsHadoopException: Could not write all entries for bulk operation [1/129]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: version_conflict_engine_exception: [9a795f736282d6baf6389d3a303008f5f31e1c163635623c57b677644af3dbb0]: version conflict, required seqNo [638], primary term [1]. current document has seqNo [733] and primary term [1]
	{"update":{"_id":"9a795f736282d6baf6389d3a303008f5f31e1c163635623c57b677644af3dbb0"}}
{"doc_as_upsert":true,"doc":{"id":"0ec9dade-c0bb-4b47-bab2-c258ed304a30","blank_vote":false,"political_party":"Gato Unido","province_name":"Málaga","province_iso_code":"ES-MA","autonomic_region_name":"Andalucía","autonomic_region_iso_code":"ES-AN","location":"36.721300,-4.421400","timestamp":1767474008767,"doc_id":"9a795f736282d6baf6389d3a303008f5f31e1c163635623c57b677644af3dbb0"}}

Bailing out...
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.flush(BulkProcessor.java:538)
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.close(BulkProcessor.java:560)
		at org.elasticsearch.hadoop.rest.RestRepository.close(RestRepository.java:226)
		at org.elasticsearch.hadoop.rest.RestService$PartitionWriter.close(RestService.java:122)
		at org.elasticsearch.spark.rdd.EsRDDWriter$$anon$1.onTaskCompletion(EsRDDWriter.scala:74)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)
		... 12 more

2026-01-03 23:10:24,608 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-03 23:10:24,623 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 23:10:24,623 | INFO     | py4j.clientserver | Error while python server was waiting fora message
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 566, in wait_for_commands
    command = smart_decode(self.stream.readline())[:-1]
                           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-03 23:10:24,623 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-01-03 23:10:24,623 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 23:10:24,623 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 23:10:24,623 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 23:14:58,539 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-03 23:15:05,622 | INFO     | py4j.java_gateway | Callback Server Starting
2026-01-03 23:15:05,622 | INFO     | py4j.java_gateway | Socket listening on ('127.0.0.1', 50807)
2026-01-03 23:15:13,329 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-03 23:15:13,341 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 23:15:13,342 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-01-03 23:15:13,348 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 23:15:13,349 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 23:18:08,920 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-03 23:18:15,261 | INFO     | py4j.java_gateway | Callback Server Starting
2026-01-03 23:18:15,261 | INFO     | py4j.java_gateway | Socket listening on ('127.0.0.1', 51074)
2026-01-03 23:18:54,267 | INFO     | py4j.clientserver | Python Server ready to receive messages
2026-01-03 23:18:54,272 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-03 23:18:54,661 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 0 - Count: 0
2026-01-03 23:19:52,973 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-03 23:19:53,281 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 1 - Count: 5
2026-01-03 23:19:53,984 | ERROR    | py4j.clientserver | There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 120, in call
    raise e
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "D:\dev\UCM-BD_DE\votes_manager\streaming\jobs\votes_streaming_job.py", line 231, in _write_to_elasticsearch
    .save()
     ^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1461, in save
    self._jwrite.save()
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o147.save.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 415) (DESKTOP-HMVAKG6.mshome.net executor driver): org.apache.spark.util.TaskCompletionListenerException: Could not write all entries for bulk operation [1/1]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: version_conflict_engine_exception: [9c2fe97b78f88341942f59bd463891ca397fb1b4f5a5120fd1ce4c9d6e9837aa]: version conflict, document already exists (current version [1])
	{"update":{"_id":"9c2fe97b78f88341942f59bd463891ca397fb1b4f5a5120fd1ce4c9d6e9837aa"}}
{"doc_as_upsert":true,"doc":{"id":"c564f8af-9009-4472-9afb-3f9e2acd903d","blank_vote":false,"political_party":"Pepino Social","province_name":"León","province_iso_code":"ES-LE","autonomic_region_name":"Castilla y León","autonomic_region_iso_code":"ES-CL","location":"42.598700,-5.567100","timestamp":1767475154337,"doc_id":"9c2fe97b78f88341942f59bd463891ca397fb1b4f5a5120fd1ce4c9d6e9837aa"}}

Bailing out...
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:254)
	at org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:185)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
	Suppressed: org.elasticsearch.hadoop.EsHadoopException: Could not write all entries for bulk operation [1/1]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: version_conflict_engine_exception: [9c2fe97b78f88341942f59bd463891ca397fb1b4f5a5120fd1ce4c9d6e9837aa]: version conflict, document already exists (current version [1])
	{"update":{"_id":"9c2fe97b78f88341942f59bd463891ca397fb1b4f5a5120fd1ce4c9d6e9837aa"}}
{"doc_as_upsert":true,"doc":{"id":"c564f8af-9009-4472-9afb-3f9e2acd903d","blank_vote":false,"political_party":"Pepino Social","province_name":"León","province_iso_code":"ES-LE","autonomic_region_name":"Castilla y León","autonomic_region_iso_code":"ES-CL","location":"42.598700,-5.567100","timestamp":1767475154337,"doc_id":"9c2fe97b78f88341942f59bd463891ca397fb1b4f5a5120fd1ce4c9d6e9837aa"}}

Bailing out...
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.flush(BulkProcessor.java:538)
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.close(BulkProcessor.java:560)
		at org.elasticsearch.hadoop.rest.RestRepository.close(RestRepository.java:226)
		at org.elasticsearch.hadoop.rest.RestService$PartitionWriter.close(RestService.java:122)
		at org.elasticsearch.spark.rdd.EsRDDWriter$$anon$1.onTaskCompletion(EsRDDWriter.scala:74)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)
		... 12 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2898)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2834)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2833)
	at scala.collection.immutable.List.foreach(List.scala:333)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2833)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1253)
	at scala.Option.foreach(Option.scala:437)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3102)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3036)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3025)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:995)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2446)
	at org.elasticsearch.spark.sql.EsSparkSQL$.saveToEs(EsSparkSQL.scala:103)
	at org.elasticsearch.spark.sql.ElasticsearchRelation.insert(DefaultSource.scala:629)
	at org.elasticsearch.spark.sql.DefaultSource.createRelation(DefaultSource.scala:107)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy34.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.spark.util.TaskCompletionListenerException: Could not write all entries for bulk operation [1/1]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: version_conflict_engine_exception: [9c2fe97b78f88341942f59bd463891ca397fb1b4f5a5120fd1ce4c9d6e9837aa]: version conflict, document already exists (current version [1])
	{"update":{"_id":"9c2fe97b78f88341942f59bd463891ca397fb1b4f5a5120fd1ce4c9d6e9837aa"}}
{"doc_as_upsert":true,"doc":{"id":"c564f8af-9009-4472-9afb-3f9e2acd903d","blank_vote":false,"political_party":"Pepino Social","province_name":"León","province_iso_code":"ES-LE","autonomic_region_name":"Castilla y León","autonomic_region_iso_code":"ES-CL","location":"42.598700,-5.567100","timestamp":1767475154337,"doc_id":"9c2fe97b78f88341942f59bd463891ca397fb1b4f5a5120fd1ce4c9d6e9837aa"}}

Bailing out...
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:254)
	at org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:185)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
	Suppressed: org.elasticsearch.hadoop.EsHadoopException: Could not write all entries for bulk operation [1/1]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: version_conflict_engine_exception: [9c2fe97b78f88341942f59bd463891ca397fb1b4f5a5120fd1ce4c9d6e9837aa]: version conflict, document already exists (current version [1])
	{"update":{"_id":"9c2fe97b78f88341942f59bd463891ca397fb1b4f5a5120fd1ce4c9d6e9837aa"}}
{"doc_as_upsert":true,"doc":{"id":"c564f8af-9009-4472-9afb-3f9e2acd903d","blank_vote":false,"political_party":"Pepino Social","province_name":"León","province_iso_code":"ES-LE","autonomic_region_name":"Castilla y León","autonomic_region_iso_code":"ES-CL","location":"42.598700,-5.567100","timestamp":1767475154337,"doc_id":"9c2fe97b78f88341942f59bd463891ca397fb1b4f5a5120fd1ce4c9d6e9837aa"}}

Bailing out...
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.flush(BulkProcessor.java:538)
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.close(BulkProcessor.java:560)
		at org.elasticsearch.hadoop.rest.RestRepository.close(RestRepository.java:226)
		at org.elasticsearch.hadoop.rest.RestService$PartitionWriter.close(RestService.java:122)
		at org.elasticsearch.spark.rdd.EsRDDWriter$$anon$1.onTaskCompletion(EsRDDWriter.scala:74)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)
		... 12 more

2026-01-03 23:21:26,121 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-03 23:21:26,127 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 23:21:26,128 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-01-03 23:21:26,132 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 23:25:33,189 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-03 23:25:39,553 | INFO     | py4j.java_gateway | Callback Server Starting
2026-01-03 23:25:39,553 | INFO     | py4j.java_gateway | Socket listening on ('127.0.0.1', 51257)
2026-01-03 23:26:20,271 | INFO     | py4j.clientserver | Python Server ready to receive messages
2026-01-03 23:26:20,272 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-03 23:26:20,771 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 0 - Count: 0
2026-01-03 23:27:38,651 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-03 23:27:38,923 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 1 - Count: 4
2026-01-03 23:28:23,965 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-03 23:29:02,163 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 2 - Count: 98
2026-01-03 23:29:05,368 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-03 23:29:05,444 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 3 - Count: 0
2026-01-03 23:48:17,407 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-03 23:48:17,421 | INFO     | py4j.clientserver | Error while python server was waiting fora message
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 566, in wait_for_commands
    command = smart_decode(self.stream.readline())[:-1]
                           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-03 23:48:17,423 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 23:48:17,424 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 23:48:17,425 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-01-03 23:48:17,429 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 23:48:17,429 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 23:51:19,177 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-03 23:51:25,219 | INFO     | py4j.java_gateway | Callback Server Starting
2026-01-03 23:51:25,219 | INFO     | py4j.java_gateway | Socket listening on ('127.0.0.1', 51541)
2026-01-03 23:52:03,010 | INFO     | py4j.clientserver | Python Server ready to receive messages
2026-01-03 23:52:03,011 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-03 23:52:03,466 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 0 - Count: 0
2026-01-03 23:55:21,703 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-03 23:55:22,063 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 1 - Count: 5
2026-01-03 23:55:22,750 | ERROR    | py4j.clientserver | There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 120, in call
    raise e
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "D:\dev\UCM-BD_DE\votes_manager\streaming\jobs\votes_streaming_job.py", line 232, in _write_to_elasticsearch
    .save()
     ^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1461, in save
    self._jwrite.save()
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o150.save.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 12.0 failed 1 times, most recent failure: Lost task 1.0 in stage 12.0 (TID 416) (DESKTOP-HMVAKG6.mshome.net executor driver): org.apache.spark.util.TaskCompletionListenerException: Could not write all entries for bulk operation [1/1]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: document_parsing_exception: [1:275] failed to parse field [timestamp] of type [date] in document with id 'df493ab00ee21f02678505bb06aa52276fa944be97f184dc400cecb8e37d504c'. Preview of field's value: '1767477286412';org.elasticsearch.hadoop.rest.EsHadoopRemoteException: illegal_argument_exception: failed to parse date field [1767477286412] with format [yyyy-MM-dd HH:mm:ss.SSSSSS];org.elasticsearch.hadoop.rest.EsHadoopRemoteException: date_time_parse_exception: Text '1767477286412' could not be parsed at index 0
	{"update":{"_id":"df493ab00ee21f02678505bb06aa52276fa944be97f184dc400cecb8e37d504c"}}
{"doc_as_upsert":true,"doc":{"id":"4e86523b-4074-4d9a-98af-dd13df009ccd","blank_vote":false,"political_party":"Gato Unido","province_name":"Madrid","province_iso_code":"ES-M","autonomic_region_name":"Comunidad de Madrid","autonomic_region_iso_code":"ES-MD","location":"40.416800,-3.703800","timestamp":1767477286412,"doc_id":"df493ab00ee21f02678505bb06aa52276fa944be97f184dc400cecb8e37d504c"}}

Bailing out...
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:254)
	at org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:185)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
	Suppressed: org.elasticsearch.hadoop.EsHadoopException: Could not write all entries for bulk operation [1/1]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: document_parsing_exception: [1:275] failed to parse field [timestamp] of type [date] in document with id 'df493ab00ee21f02678505bb06aa52276fa944be97f184dc400cecb8e37d504c'. Preview of field's value: '1767477286412';org.elasticsearch.hadoop.rest.EsHadoopRemoteException: illegal_argument_exception: failed to parse date field [1767477286412] with format [yyyy-MM-dd HH:mm:ss.SSSSSS];org.elasticsearch.hadoop.rest.EsHadoopRemoteException: date_time_parse_exception: Text '1767477286412' could not be parsed at index 0
	{"update":{"_id":"df493ab00ee21f02678505bb06aa52276fa944be97f184dc400cecb8e37d504c"}}
{"doc_as_upsert":true,"doc":{"id":"4e86523b-4074-4d9a-98af-dd13df009ccd","blank_vote":false,"political_party":"Gato Unido","province_name":"Madrid","province_iso_code":"ES-M","autonomic_region_name":"Comunidad de Madrid","autonomic_region_iso_code":"ES-MD","location":"40.416800,-3.703800","timestamp":1767477286412,"doc_id":"df493ab00ee21f02678505bb06aa52276fa944be97f184dc400cecb8e37d504c"}}

Bailing out...
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.flush(BulkProcessor.java:538)
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.close(BulkProcessor.java:560)
		at org.elasticsearch.hadoop.rest.RestRepository.close(RestRepository.java:226)
		at org.elasticsearch.hadoop.rest.RestService$PartitionWriter.close(RestService.java:122)
		at org.elasticsearch.spark.rdd.EsRDDWriter$$anon$1.onTaskCompletion(EsRDDWriter.scala:74)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)
		... 12 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2898)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2834)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2833)
	at scala.collection.immutable.List.foreach(List.scala:333)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2833)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1253)
	at scala.Option.foreach(Option.scala:437)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3102)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3036)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3025)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:995)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2446)
	at org.elasticsearch.spark.sql.EsSparkSQL$.saveToEs(EsSparkSQL.scala:103)
	at org.elasticsearch.spark.sql.ElasticsearchRelation.insert(DefaultSource.scala:629)
	at org.elasticsearch.spark.sql.DefaultSource.createRelation(DefaultSource.scala:107)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy34.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.spark.util.TaskCompletionListenerException: Could not write all entries for bulk operation [1/1]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: document_parsing_exception: [1:275] failed to parse field [timestamp] of type [date] in document with id 'df493ab00ee21f02678505bb06aa52276fa944be97f184dc400cecb8e37d504c'. Preview of field's value: '1767477286412';org.elasticsearch.hadoop.rest.EsHadoopRemoteException: illegal_argument_exception: failed to parse date field [1767477286412] with format [yyyy-MM-dd HH:mm:ss.SSSSSS];org.elasticsearch.hadoop.rest.EsHadoopRemoteException: date_time_parse_exception: Text '1767477286412' could not be parsed at index 0
	{"update":{"_id":"df493ab00ee21f02678505bb06aa52276fa944be97f184dc400cecb8e37d504c"}}
{"doc_as_upsert":true,"doc":{"id":"4e86523b-4074-4d9a-98af-dd13df009ccd","blank_vote":false,"political_party":"Gato Unido","province_name":"Madrid","province_iso_code":"ES-M","autonomic_region_name":"Comunidad de Madrid","autonomic_region_iso_code":"ES-MD","location":"40.416800,-3.703800","timestamp":1767477286412,"doc_id":"df493ab00ee21f02678505bb06aa52276fa944be97f184dc400cecb8e37d504c"}}

Bailing out...
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:254)
	at org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:185)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
	Suppressed: org.elasticsearch.hadoop.EsHadoopException: Could not write all entries for bulk operation [1/1]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: document_parsing_exception: [1:275] failed to parse field [timestamp] of type [date] in document with id 'df493ab00ee21f02678505bb06aa52276fa944be97f184dc400cecb8e37d504c'. Preview of field's value: '1767477286412';org.elasticsearch.hadoop.rest.EsHadoopRemoteException: illegal_argument_exception: failed to parse date field [1767477286412] with format [yyyy-MM-dd HH:mm:ss.SSSSSS];org.elasticsearch.hadoop.rest.EsHadoopRemoteException: date_time_parse_exception: Text '1767477286412' could not be parsed at index 0
	{"update":{"_id":"df493ab00ee21f02678505bb06aa52276fa944be97f184dc400cecb8e37d504c"}}
{"doc_as_upsert":true,"doc":{"id":"4e86523b-4074-4d9a-98af-dd13df009ccd","blank_vote":false,"political_party":"Gato Unido","province_name":"Madrid","province_iso_code":"ES-M","autonomic_region_name":"Comunidad de Madrid","autonomic_region_iso_code":"ES-MD","location":"40.416800,-3.703800","timestamp":1767477286412,"doc_id":"df493ab00ee21f02678505bb06aa52276fa944be97f184dc400cecb8e37d504c"}}

Bailing out...
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.flush(BulkProcessor.java:538)
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.close(BulkProcessor.java:560)
		at org.elasticsearch.hadoop.rest.RestRepository.close(RestRepository.java:226)
		at org.elasticsearch.hadoop.rest.RestService$PartitionWriter.close(RestService.java:122)
		at org.elasticsearch.spark.rdd.EsRDDWriter$$anon$1.onTaskCompletion(EsRDDWriter.scala:74)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)
		... 12 more

2026-01-03 23:56:04,780 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-03 23:56:04,780 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 23:56:04,780 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-01-03 23:56:04,792 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-03 23:56:04,792 | INFO     | py4j.clientserver | Error while python server was waiting fora message
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 566, in wait_for_commands
    command = smart_decode(self.stream.readline())[:-1]
                           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-03 23:57:36,939 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-03 23:57:43,373 | INFO     | py4j.java_gateway | Callback Server Starting
2026-01-03 23:57:43,373 | INFO     | py4j.java_gateway | Socket listening on ('127.0.0.1', 51715)
2026-01-03 23:58:22,956 | INFO     | py4j.clientserver | Python Server ready to receive messages
2026-01-03 23:58:22,958 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-03 23:58:23,411 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 0 - Count: 0
2026-01-04 00:02:33,813 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-04 00:02:34,004 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 1 - Count: 2
2026-01-04 00:02:34,554 | ERROR    | py4j.clientserver | There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 120, in call
    raise e
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "D:\dev\UCM-BD_DE\votes_manager\streaming\jobs\votes_streaming_job.py", line 232, in _write_to_elasticsearch
    .save()
     ^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1461, in save
    self._jwrite.save()
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o150.save.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 12.0 failed 1 times, most recent failure: Lost task 1.0 in stage 12.0 (TID 413) (DESKTOP-HMVAKG6.mshome.net executor driver): org.apache.spark.util.TaskCompletionListenerException: Could not write all entries for bulk operation [1/1]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: document_parsing_exception: [1:270] failed to parse field [timestamp] of type [date] in document with id 'bcff957671d5f6f5ee430ac0b0fb9b18fcb6abbf42440cdb2fc6cd5306ed2dd8'. Preview of field's value: '1767477716952';org.elasticsearch.hadoop.rest.EsHadoopRemoteException: illegal_argument_exception: failed to parse date field [1767477716952] with format [yyyy-MM-dd HH:mm:ss.SSSSSS];org.elasticsearch.hadoop.rest.EsHadoopRemoteException: date_time_parse_exception: Text '1767477716952' could not be parsed at index 0
	{"update":{"_id":"bcff957671d5f6f5ee430ac0b0fb9b18fcb6abbf42440cdb2fc6cd5306ed2dd8"}}
{"doc_as_upsert":true,"doc":{"id":"6c502c18-b52e-4c55-9a3a-682905628cdd","blank_vote":false,"political_party":"Perro Liberal","province_name":"Barcelona","province_iso_code":"ES-B","autonomic_region_name":"Cataluña","autonomic_region_iso_code":"ES-CT","location":"41.388800,2.159000","timestamp":1767477716952,"doc_id":"bcff957671d5f6f5ee430ac0b0fb9b18fcb6abbf42440cdb2fc6cd5306ed2dd8"}}

Bailing out...
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:254)
	at org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:185)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
	Suppressed: org.elasticsearch.hadoop.EsHadoopException: Could not write all entries for bulk operation [1/1]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: document_parsing_exception: [1:270] failed to parse field [timestamp] of type [date] in document with id 'bcff957671d5f6f5ee430ac0b0fb9b18fcb6abbf42440cdb2fc6cd5306ed2dd8'. Preview of field's value: '1767477716952';org.elasticsearch.hadoop.rest.EsHadoopRemoteException: illegal_argument_exception: failed to parse date field [1767477716952] with format [yyyy-MM-dd HH:mm:ss.SSSSSS];org.elasticsearch.hadoop.rest.EsHadoopRemoteException: date_time_parse_exception: Text '1767477716952' could not be parsed at index 0
	{"update":{"_id":"bcff957671d5f6f5ee430ac0b0fb9b18fcb6abbf42440cdb2fc6cd5306ed2dd8"}}
{"doc_as_upsert":true,"doc":{"id":"6c502c18-b52e-4c55-9a3a-682905628cdd","blank_vote":false,"political_party":"Perro Liberal","province_name":"Barcelona","province_iso_code":"ES-B","autonomic_region_name":"Cataluña","autonomic_region_iso_code":"ES-CT","location":"41.388800,2.159000","timestamp":1767477716952,"doc_id":"bcff957671d5f6f5ee430ac0b0fb9b18fcb6abbf42440cdb2fc6cd5306ed2dd8"}}

Bailing out...
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.flush(BulkProcessor.java:538)
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.close(BulkProcessor.java:560)
		at org.elasticsearch.hadoop.rest.RestRepository.close(RestRepository.java:226)
		at org.elasticsearch.hadoop.rest.RestService$PartitionWriter.close(RestService.java:122)
		at org.elasticsearch.spark.rdd.EsRDDWriter$$anon$1.onTaskCompletion(EsRDDWriter.scala:74)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)
		... 12 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2898)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2834)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2833)
	at scala.collection.immutable.List.foreach(List.scala:333)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2833)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1253)
	at scala.Option.foreach(Option.scala:437)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3102)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3036)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3025)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:995)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2446)
	at org.elasticsearch.spark.sql.EsSparkSQL$.saveToEs(EsSparkSQL.scala:103)
	at org.elasticsearch.spark.sql.ElasticsearchRelation.insert(DefaultSource.scala:629)
	at org.elasticsearch.spark.sql.DefaultSource.createRelation(DefaultSource.scala:107)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy34.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.spark.util.TaskCompletionListenerException: Could not write all entries for bulk operation [1/1]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: document_parsing_exception: [1:270] failed to parse field [timestamp] of type [date] in document with id 'bcff957671d5f6f5ee430ac0b0fb9b18fcb6abbf42440cdb2fc6cd5306ed2dd8'. Preview of field's value: '1767477716952';org.elasticsearch.hadoop.rest.EsHadoopRemoteException: illegal_argument_exception: failed to parse date field [1767477716952] with format [yyyy-MM-dd HH:mm:ss.SSSSSS];org.elasticsearch.hadoop.rest.EsHadoopRemoteException: date_time_parse_exception: Text '1767477716952' could not be parsed at index 0
	{"update":{"_id":"bcff957671d5f6f5ee430ac0b0fb9b18fcb6abbf42440cdb2fc6cd5306ed2dd8"}}
{"doc_as_upsert":true,"doc":{"id":"6c502c18-b52e-4c55-9a3a-682905628cdd","blank_vote":false,"political_party":"Perro Liberal","province_name":"Barcelona","province_iso_code":"ES-B","autonomic_region_name":"Cataluña","autonomic_region_iso_code":"ES-CT","location":"41.388800,2.159000","timestamp":1767477716952,"doc_id":"bcff957671d5f6f5ee430ac0b0fb9b18fcb6abbf42440cdb2fc6cd5306ed2dd8"}}

Bailing out...
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:254)
	at org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:185)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
	Suppressed: org.elasticsearch.hadoop.EsHadoopException: Could not write all entries for bulk operation [1/1]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: document_parsing_exception: [1:270] failed to parse field [timestamp] of type [date] in document with id 'bcff957671d5f6f5ee430ac0b0fb9b18fcb6abbf42440cdb2fc6cd5306ed2dd8'. Preview of field's value: '1767477716952';org.elasticsearch.hadoop.rest.EsHadoopRemoteException: illegal_argument_exception: failed to parse date field [1767477716952] with format [yyyy-MM-dd HH:mm:ss.SSSSSS];org.elasticsearch.hadoop.rest.EsHadoopRemoteException: date_time_parse_exception: Text '1767477716952' could not be parsed at index 0
	{"update":{"_id":"bcff957671d5f6f5ee430ac0b0fb9b18fcb6abbf42440cdb2fc6cd5306ed2dd8"}}
{"doc_as_upsert":true,"doc":{"id":"6c502c18-b52e-4c55-9a3a-682905628cdd","blank_vote":false,"political_party":"Perro Liberal","province_name":"Barcelona","province_iso_code":"ES-B","autonomic_region_name":"Cataluña","autonomic_region_iso_code":"ES-CT","location":"41.388800,2.159000","timestamp":1767477716952,"doc_id":"bcff957671d5f6f5ee430ac0b0fb9b18fcb6abbf42440cdb2fc6cd5306ed2dd8"}}

Bailing out...
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.flush(BulkProcessor.java:538)
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.close(BulkProcessor.java:560)
		at org.elasticsearch.hadoop.rest.RestRepository.close(RestRepository.java:226)
		at org.elasticsearch.hadoop.rest.RestService$PartitionWriter.close(RestService.java:122)
		at org.elasticsearch.spark.rdd.EsRDDWriter$$anon$1.onTaskCompletion(EsRDDWriter.scala:74)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)
		... 12 more

2026-01-04 00:02:46,313 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-04 00:02:46,313 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-04 00:02:46,313 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-01-04 00:02:46,324 | INFO     | py4j.clientserver | Error while python server was waiting fora message
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 566, in wait_for_commands
    command = smart_decode(self.stream.readline())[:-1]
                           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-04 00:02:46,325 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-04 00:02:46,325 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-04 00:02:46,327 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-05 19:43:12,833 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-05 19:43:21,786 | INFO     | py4j.java_gateway | Callback Server Starting
2026-01-05 19:43:21,786 | INFO     | py4j.java_gateway | Socket listening on ('127.0.0.1', 56845)
2026-01-05 19:44:17,116 | INFO     | py4j.clientserver | Python Server ready to receive messages
2026-01-05 19:44:17,117 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-05 19:44:17,764 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 0 - Count: 0
2026-01-05 19:47:29,579 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-05 19:47:29,782 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 1 - Count: 4
2026-01-05 19:47:30,873 | ERROR    | py4j.clientserver | There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 120, in call
    raise e
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "D:\dev\UCM-BD_DE\votes_manager\streaming\jobs\votes_streaming_job.py", line 232, in _write_to_elasticsearch
    .save()
     ^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1461, in save
    self._jwrite.save()
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o150.save.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 12.0 failed 1 times, most recent failure: Lost task 1.0 in stage 12.0 (TID 415) (DESKTOP-HMVAKG6.mshome.net executor driver): org.apache.spark.util.TaskCompletionListenerException: Could not write all entries for bulk operation [1/1]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: document_parsing_exception: [1:275] failed to parse field [timestamp] of type [date] in document with id '02b8df86d77f23f0b921943d92377acdeb191b46839dcbab4c5395bf81279ad0'. Preview of field's value: '1767635200401';org.elasticsearch.hadoop.rest.EsHadoopRemoteException: illegal_argument_exception: failed to parse date field [1767635200401] with format [yyyy-MM-dd HH:mm:ss.SSSSSS];org.elasticsearch.hadoop.rest.EsHadoopRemoteException: date_time_parse_exception: Text '1767635200401' could not be parsed at index 0
	{"update":{"_id":"02b8df86d77f23f0b921943d92377acdeb191b46839dcbab4c5395bf81279ad0"}}
{"doc_as_upsert":true,"doc":{"id":"25284664-81a5-436c-af20-a7428c417469","blank_vote":false,"political_party":"Gato Unido","province_name":"Toledo","province_iso_code":"ES-TO","autonomic_region_name":"Castilla La Mancha","autonomic_region_iso_code":"ES-CM","location":"39.862800,-4.027300","timestamp":1767635200401,"doc_id":"02b8df86d77f23f0b921943d92377acdeb191b46839dcbab4c5395bf81279ad0"}}

Bailing out...
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:254)
	at org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:185)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
	Suppressed: org.elasticsearch.hadoop.EsHadoopException: Could not write all entries for bulk operation [1/1]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: document_parsing_exception: [1:275] failed to parse field [timestamp] of type [date] in document with id '02b8df86d77f23f0b921943d92377acdeb191b46839dcbab4c5395bf81279ad0'. Preview of field's value: '1767635200401';org.elasticsearch.hadoop.rest.EsHadoopRemoteException: illegal_argument_exception: failed to parse date field [1767635200401] with format [yyyy-MM-dd HH:mm:ss.SSSSSS];org.elasticsearch.hadoop.rest.EsHadoopRemoteException: date_time_parse_exception: Text '1767635200401' could not be parsed at index 0
	{"update":{"_id":"02b8df86d77f23f0b921943d92377acdeb191b46839dcbab4c5395bf81279ad0"}}
{"doc_as_upsert":true,"doc":{"id":"25284664-81a5-436c-af20-a7428c417469","blank_vote":false,"political_party":"Gato Unido","province_name":"Toledo","province_iso_code":"ES-TO","autonomic_region_name":"Castilla La Mancha","autonomic_region_iso_code":"ES-CM","location":"39.862800,-4.027300","timestamp":1767635200401,"doc_id":"02b8df86d77f23f0b921943d92377acdeb191b46839dcbab4c5395bf81279ad0"}}

Bailing out...
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.flush(BulkProcessor.java:538)
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.close(BulkProcessor.java:560)
		at org.elasticsearch.hadoop.rest.RestRepository.close(RestRepository.java:226)
		at org.elasticsearch.hadoop.rest.RestService$PartitionWriter.close(RestService.java:122)
		at org.elasticsearch.spark.rdd.EsRDDWriter$$anon$1.onTaskCompletion(EsRDDWriter.scala:74)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)
		... 12 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2898)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2834)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2833)
	at scala.collection.immutable.List.foreach(List.scala:333)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2833)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1253)
	at scala.Option.foreach(Option.scala:437)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3102)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3036)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3025)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:995)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2446)
	at org.elasticsearch.spark.sql.EsSparkSQL$.saveToEs(EsSparkSQL.scala:103)
	at org.elasticsearch.spark.sql.ElasticsearchRelation.insert(DefaultSource.scala:629)
	at org.elasticsearch.spark.sql.DefaultSource.createRelation(DefaultSource.scala:107)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy34.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.spark.util.TaskCompletionListenerException: Could not write all entries for bulk operation [1/1]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: document_parsing_exception: [1:275] failed to parse field [timestamp] of type [date] in document with id '02b8df86d77f23f0b921943d92377acdeb191b46839dcbab4c5395bf81279ad0'. Preview of field's value: '1767635200401';org.elasticsearch.hadoop.rest.EsHadoopRemoteException: illegal_argument_exception: failed to parse date field [1767635200401] with format [yyyy-MM-dd HH:mm:ss.SSSSSS];org.elasticsearch.hadoop.rest.EsHadoopRemoteException: date_time_parse_exception: Text '1767635200401' could not be parsed at index 0
	{"update":{"_id":"02b8df86d77f23f0b921943d92377acdeb191b46839dcbab4c5395bf81279ad0"}}
{"doc_as_upsert":true,"doc":{"id":"25284664-81a5-436c-af20-a7428c417469","blank_vote":false,"political_party":"Gato Unido","province_name":"Toledo","province_iso_code":"ES-TO","autonomic_region_name":"Castilla La Mancha","autonomic_region_iso_code":"ES-CM","location":"39.862800,-4.027300","timestamp":1767635200401,"doc_id":"02b8df86d77f23f0b921943d92377acdeb191b46839dcbab4c5395bf81279ad0"}}

Bailing out...
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:254)
	at org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:185)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
	Suppressed: org.elasticsearch.hadoop.EsHadoopException: Could not write all entries for bulk operation [1/1]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: document_parsing_exception: [1:275] failed to parse field [timestamp] of type [date] in document with id '02b8df86d77f23f0b921943d92377acdeb191b46839dcbab4c5395bf81279ad0'. Preview of field's value: '1767635200401';org.elasticsearch.hadoop.rest.EsHadoopRemoteException: illegal_argument_exception: failed to parse date field [1767635200401] with format [yyyy-MM-dd HH:mm:ss.SSSSSS];org.elasticsearch.hadoop.rest.EsHadoopRemoteException: date_time_parse_exception: Text '1767635200401' could not be parsed at index 0
	{"update":{"_id":"02b8df86d77f23f0b921943d92377acdeb191b46839dcbab4c5395bf81279ad0"}}
{"doc_as_upsert":true,"doc":{"id":"25284664-81a5-436c-af20-a7428c417469","blank_vote":false,"political_party":"Gato Unido","province_name":"Toledo","province_iso_code":"ES-TO","autonomic_region_name":"Castilla La Mancha","autonomic_region_iso_code":"ES-CM","location":"39.862800,-4.027300","timestamp":1767635200401,"doc_id":"02b8df86d77f23f0b921943d92377acdeb191b46839dcbab4c5395bf81279ad0"}}

Bailing out...
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.flush(BulkProcessor.java:538)
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.close(BulkProcessor.java:560)
		at org.elasticsearch.hadoop.rest.RestRepository.close(RestRepository.java:226)
		at org.elasticsearch.hadoop.rest.RestService$PartitionWriter.close(RestService.java:122)
		at org.elasticsearch.spark.rdd.EsRDDWriter$$anon$1.onTaskCompletion(EsRDDWriter.scala:74)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)
		... 12 more

2026-01-05 19:49:49,016 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-05 19:49:49,018 | INFO     | py4j.clientserver | Error while python server was waiting fora message
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 566, in wait_for_commands
    command = smart_decode(self.stream.readline())[:-1]
                           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-05 19:49:49,066 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-05 19:49:49,067 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-05 19:49:49,068 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-01-05 19:49:49,069 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-05 19:49:49,070 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-05 19:51:33,239 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-05 19:51:41,081 | INFO     | py4j.java_gateway | Callback Server Starting
2026-01-05 19:51:41,081 | INFO     | py4j.java_gateway | Socket listening on ('127.0.0.1', 57192)
2026-01-05 19:51:42,974 | INFO     | py4j.clientserver | Python Server ready to receive messages
2026-01-05 19:51:42,975 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-05 19:51:45,064 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 1 - Count: 4
2026-01-05 19:52:30,383 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-05 19:52:31,010 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 2 - Count: 98
2026-01-05 19:52:32,823 | ERROR    | py4j.clientserver | There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 120, in call
    raise e
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "D:\dev\UCM-BD_DE\votes_manager\streaming\jobs\votes_streaming_job.py", line 232, in _write_to_elasticsearch
    .save()
     ^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1461, in save
    self._jwrite.save()
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o150.save.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 9.0 failed 1 times, most recent failure: Lost task 2.0 in stage 9.0 (TID 305) (DESKTOP-HMVAKG6.mshome.net executor driver): org.apache.spark.util.TaskCompletionListenerException: Could not write all entries for bulk operation [1/13]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: version_conflict_engine_exception: [06c6fc87616afb803c1fbfa09620b9d0c33a1d8610e131c04fb23a787d6180c9]: version conflict, required seqNo [66], primary term [1]. current document has seqNo [78] and primary term [1]
	{"update":{"_id":"06c6fc87616afb803c1fbfa09620b9d0c33a1d8610e131c04fb23a787d6180c9"}}
{"doc_as_upsert":true,"doc":{"id":"9d9f9147-54c9-4907-9066-f80aecaa8126","blank_vote":false,"political_party":"Perro Liberal","province_name":"Madrid","province_iso_code":"ES-M","autonomic_region_name":"Comunidad de Madrid","autonomic_region_iso_code":"ES-MD","location":"40.416800,-3.703800","timestamp":1767635201309,"doc_id":"06c6fc87616afb803c1fbfa09620b9d0c33a1d8610e131c04fb23a787d6180c9"}}

Bailing out...
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:254)
	at org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:185)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
	Suppressed: org.elasticsearch.hadoop.EsHadoopException: Could not write all entries for bulk operation [1/13]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: version_conflict_engine_exception: [06c6fc87616afb803c1fbfa09620b9d0c33a1d8610e131c04fb23a787d6180c9]: version conflict, required seqNo [66], primary term [1]. current document has seqNo [78] and primary term [1]
	{"update":{"_id":"06c6fc87616afb803c1fbfa09620b9d0c33a1d8610e131c04fb23a787d6180c9"}}
{"doc_as_upsert":true,"doc":{"id":"9d9f9147-54c9-4907-9066-f80aecaa8126","blank_vote":false,"political_party":"Perro Liberal","province_name":"Madrid","province_iso_code":"ES-M","autonomic_region_name":"Comunidad de Madrid","autonomic_region_iso_code":"ES-MD","location":"40.416800,-3.703800","timestamp":1767635201309,"doc_id":"06c6fc87616afb803c1fbfa09620b9d0c33a1d8610e131c04fb23a787d6180c9"}}

Bailing out...
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.flush(BulkProcessor.java:538)
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.close(BulkProcessor.java:560)
		at org.elasticsearch.hadoop.rest.RestRepository.close(RestRepository.java:226)
		at org.elasticsearch.hadoop.rest.RestService$PartitionWriter.close(RestService.java:122)
		at org.elasticsearch.spark.rdd.EsRDDWriter$$anon$1.onTaskCompletion(EsRDDWriter.scala:74)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)
		... 12 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2898)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2834)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2833)
	at scala.collection.immutable.List.foreach(List.scala:333)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2833)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1253)
	at scala.Option.foreach(Option.scala:437)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3102)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3036)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3025)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:995)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2446)
	at org.elasticsearch.spark.sql.EsSparkSQL$.saveToEs(EsSparkSQL.scala:103)
	at org.elasticsearch.spark.sql.ElasticsearchRelation.insert(DefaultSource.scala:629)
	at org.elasticsearch.spark.sql.DefaultSource.createRelation(DefaultSource.scala:107)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy32.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.spark.util.TaskCompletionListenerException: Could not write all entries for bulk operation [1/13]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: version_conflict_engine_exception: [06c6fc87616afb803c1fbfa09620b9d0c33a1d8610e131c04fb23a787d6180c9]: version conflict, required seqNo [66], primary term [1]. current document has seqNo [78] and primary term [1]
	{"update":{"_id":"06c6fc87616afb803c1fbfa09620b9d0c33a1d8610e131c04fb23a787d6180c9"}}
{"doc_as_upsert":true,"doc":{"id":"9d9f9147-54c9-4907-9066-f80aecaa8126","blank_vote":false,"political_party":"Perro Liberal","province_name":"Madrid","province_iso_code":"ES-M","autonomic_region_name":"Comunidad de Madrid","autonomic_region_iso_code":"ES-MD","location":"40.416800,-3.703800","timestamp":1767635201309,"doc_id":"06c6fc87616afb803c1fbfa09620b9d0c33a1d8610e131c04fb23a787d6180c9"}}

Bailing out...
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:254)
	at org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:185)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
	Suppressed: org.elasticsearch.hadoop.EsHadoopException: Could not write all entries for bulk operation [1/13]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: version_conflict_engine_exception: [06c6fc87616afb803c1fbfa09620b9d0c33a1d8610e131c04fb23a787d6180c9]: version conflict, required seqNo [66], primary term [1]. current document has seqNo [78] and primary term [1]
	{"update":{"_id":"06c6fc87616afb803c1fbfa09620b9d0c33a1d8610e131c04fb23a787d6180c9"}}
{"doc_as_upsert":true,"doc":{"id":"9d9f9147-54c9-4907-9066-f80aecaa8126","blank_vote":false,"political_party":"Perro Liberal","province_name":"Madrid","province_iso_code":"ES-M","autonomic_region_name":"Comunidad de Madrid","autonomic_region_iso_code":"ES-MD","location":"40.416800,-3.703800","timestamp":1767635201309,"doc_id":"06c6fc87616afb803c1fbfa09620b9d0c33a1d8610e131c04fb23a787d6180c9"}}

Bailing out...
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.flush(BulkProcessor.java:538)
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.close(BulkProcessor.java:560)
		at org.elasticsearch.hadoop.rest.RestRepository.close(RestRepository.java:226)
		at org.elasticsearch.hadoop.rest.RestService$PartitionWriter.close(RestService.java:122)
		at org.elasticsearch.spark.rdd.EsRDDWriter$$anon$1.onTaskCompletion(EsRDDWriter.scala:74)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)
		... 12 more

2026-01-05 19:57:18,853 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-05 19:59:07,714 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-05 19:59:15,324 | INFO     | py4j.java_gateway | Callback Server Starting
2026-01-05 19:59:15,324 | INFO     | py4j.java_gateway | Socket listening on ('127.0.0.1', 57532)
2026-01-05 20:00:00,871 | INFO     | py4j.clientserver | Python Server ready to receive messages
2026-01-05 20:00:00,871 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-05 20:00:01,385 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 0 - Count: 0
2026-01-05 20:05:29,634 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-05 20:05:29,879 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 1 - Count: 2
2026-01-05 20:06:10,739 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-05 20:06:46,989 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 2 - Count: 100
2026-01-05 20:06:49,539 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-05 20:06:49,635 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 3 - Count: 0
2026-01-05 20:14:56,154 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-05 20:14:56,156 | INFO     | py4j.clientserver | Error while python server was waiting fora message
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 566, in wait_for_commands
    command = smart_decode(self.stream.readline())[:-1]
                           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-05 20:14:56,160 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-05 20:14:56,161 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-05 20:14:56,162 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-01-05 20:14:56,168 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-05 20:14:56,169 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-05 20:18:01,884 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-05 20:18:10,054 | INFO     | py4j.java_gateway | Callback Server Starting
2026-01-05 20:18:10,054 | INFO     | py4j.java_gateway | Socket listening on ('127.0.0.1', 58187)
2026-01-05 20:18:55,587 | INFO     | py4j.clientserver | Python Server ready to receive messages
2026-01-05 20:18:55,588 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-05 20:18:56,186 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 0 - Count: 0
2026-01-05 20:20:19,546 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-05 20:20:19,792 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 1 - Count: 4
2026-01-05 20:21:03,026 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-05 20:21:56,998 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 2 - Count: 98
2026-01-05 20:22:01,369 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-05 20:22:01,439 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 3 - Count: 0
2026-01-05 20:23:44,017 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-05 20:23:44,220 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 4 - Count: 4
2026-01-05 20:24:40,545 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-05 20:25:28,426 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 5 - Count: 98
2026-01-05 20:25:33,390 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-05 20:25:33,493 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 6 - Count: 0
2026-01-05 20:28:40,975 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-05 20:28:40,980 | INFO     | py4j.clientserver | Error while python server was waiting fora message
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 566, in wait_for_commands
    command = smart_decode(self.stream.readline())[:-1]
                           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-05 20:28:40,982 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-05 20:28:40,983 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-05 20:28:40,984 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-01-05 20:28:40,988 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-05 20:28:40,989 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-05 20:30:58,224 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-05 20:31:05,770 | INFO     | py4j.java_gateway | Callback Server Starting
2026-01-05 20:31:05,771 | INFO     | py4j.java_gateway | Socket listening on ('127.0.0.1', 58666)
2026-01-05 20:31:57,801 | INFO     | py4j.clientserver | Python Server ready to receive messages
2026-01-05 20:31:57,802 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-05 20:31:58,285 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 0 - Count: 0
2026-01-05 20:33:20,964 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-05 20:33:21,234 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 1 - Count: 1
2026-01-05 20:34:07,819 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-05 20:34:49,284 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 2 - Count: 101
2026-01-05 20:34:51,739 | ERROR    | py4j.clientserver | There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 120, in call
    raise e
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "D:\dev\UCM-BD_DE\votes_manager\streaming\jobs\votes_streaming_job.py", line 231, in _write_to_elasticsearch
    .save()
     ^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1461, in save
    self._jwrite.save()
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o161.save.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 20.0 failed 1 times, most recent failure: Lost task 1.0 in stage 20.0 (TID 903) (DESKTOP-HMVAKG6.mshome.net executor driver): org.apache.spark.util.TaskCompletionListenerException: Could not write all entries for bulk operation [1/10]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: version_conflict_engine_exception: [fe2d5628ac66cc1cfc0a10257a1aa972e4218613e45eefe0267ca1fd93751199]: version conflict, required seqNo [13], primary term [1]. current document has seqNo [59] and primary term [1]
	{"update":{"_id":"fe2d5628ac66cc1cfc0a10257a1aa972e4218613e45eefe0267ca1fd93751199"}}
{"doc_as_upsert":true,"doc":{"id":"2b8c4d03-3416-4704-96fc-733ce2484ffb","blank_vote":false,"political_party":"Tiburon Popular","province_name":"Murcia","province_iso_code":"ES-MU","autonomic_region_name":"Región de Murcia","autonomic_region_iso_code":"ES-MC","location":"37.984700,-1.128000","timestamp":1767637953625,"doc_id":"fe2d5628ac66cc1cfc0a10257a1aa972e4218613e45eefe0267ca1fd93751199"}}

Bailing out...
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:254)
	at org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:185)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
	Suppressed: org.elasticsearch.hadoop.EsHadoopException: Could not write all entries for bulk operation [1/10]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: version_conflict_engine_exception: [fe2d5628ac66cc1cfc0a10257a1aa972e4218613e45eefe0267ca1fd93751199]: version conflict, required seqNo [13], primary term [1]. current document has seqNo [59] and primary term [1]
	{"update":{"_id":"fe2d5628ac66cc1cfc0a10257a1aa972e4218613e45eefe0267ca1fd93751199"}}
{"doc_as_upsert":true,"doc":{"id":"2b8c4d03-3416-4704-96fc-733ce2484ffb","blank_vote":false,"political_party":"Tiburon Popular","province_name":"Murcia","province_iso_code":"ES-MU","autonomic_region_name":"Región de Murcia","autonomic_region_iso_code":"ES-MC","location":"37.984700,-1.128000","timestamp":1767637953625,"doc_id":"fe2d5628ac66cc1cfc0a10257a1aa972e4218613e45eefe0267ca1fd93751199"}}

Bailing out...
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.flush(BulkProcessor.java:538)
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.close(BulkProcessor.java:560)
		at org.elasticsearch.hadoop.rest.RestRepository.close(RestRepository.java:226)
		at org.elasticsearch.hadoop.rest.RestService$PartitionWriter.close(RestService.java:122)
		at org.elasticsearch.spark.rdd.EsRDDWriter$$anon$1.onTaskCompletion(EsRDDWriter.scala:74)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)
		... 12 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2898)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2834)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2833)
	at scala.collection.immutable.List.foreach(List.scala:333)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2833)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1253)
	at scala.Option.foreach(Option.scala:437)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3102)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3036)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3025)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:995)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2446)
	at org.elasticsearch.spark.sql.EsSparkSQL$.saveToEs(EsSparkSQL.scala:103)
	at org.elasticsearch.spark.sql.ElasticsearchRelation.insert(DefaultSource.scala:629)
	at org.elasticsearch.spark.sql.DefaultSource.createRelation(DefaultSource.scala:107)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy34.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.spark.util.TaskCompletionListenerException: Could not write all entries for bulk operation [1/10]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: version_conflict_engine_exception: [fe2d5628ac66cc1cfc0a10257a1aa972e4218613e45eefe0267ca1fd93751199]: version conflict, required seqNo [13], primary term [1]. current document has seqNo [59] and primary term [1]
	{"update":{"_id":"fe2d5628ac66cc1cfc0a10257a1aa972e4218613e45eefe0267ca1fd93751199"}}
{"doc_as_upsert":true,"doc":{"id":"2b8c4d03-3416-4704-96fc-733ce2484ffb","blank_vote":false,"political_party":"Tiburon Popular","province_name":"Murcia","province_iso_code":"ES-MU","autonomic_region_name":"Región de Murcia","autonomic_region_iso_code":"ES-MC","location":"37.984700,-1.128000","timestamp":1767637953625,"doc_id":"fe2d5628ac66cc1cfc0a10257a1aa972e4218613e45eefe0267ca1fd93751199"}}

Bailing out...
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:254)
	at org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:185)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
	Suppressed: org.elasticsearch.hadoop.EsHadoopException: Could not write all entries for bulk operation [1/10]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: version_conflict_engine_exception: [fe2d5628ac66cc1cfc0a10257a1aa972e4218613e45eefe0267ca1fd93751199]: version conflict, required seqNo [13], primary term [1]. current document has seqNo [59] and primary term [1]
	{"update":{"_id":"fe2d5628ac66cc1cfc0a10257a1aa972e4218613e45eefe0267ca1fd93751199"}}
{"doc_as_upsert":true,"doc":{"id":"2b8c4d03-3416-4704-96fc-733ce2484ffb","blank_vote":false,"political_party":"Tiburon Popular","province_name":"Murcia","province_iso_code":"ES-MU","autonomic_region_name":"Región de Murcia","autonomic_region_iso_code":"ES-MC","location":"37.984700,-1.128000","timestamp":1767637953625,"doc_id":"fe2d5628ac66cc1cfc0a10257a1aa972e4218613e45eefe0267ca1fd93751199"}}

Bailing out...
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.flush(BulkProcessor.java:538)
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.close(BulkProcessor.java:560)
		at org.elasticsearch.hadoop.rest.RestRepository.close(RestRepository.java:226)
		at org.elasticsearch.hadoop.rest.RestService$PartitionWriter.close(RestService.java:122)
		at org.elasticsearch.spark.rdd.EsRDDWriter$$anon$1.onTaskCompletion(EsRDDWriter.scala:74)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)
		... 12 more

2026-01-05 20:35:39,336 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-05 20:35:39,336 | INFO     | py4j.clientserver | Error while python server was waiting fora message
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 566, in wait_for_commands
    command = smart_decode(self.stream.readline())[:-1]
                           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-05 20:35:39,356 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-05 20:35:39,357 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-05 20:35:39,357 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-01-05 20:35:39,357 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-05 20:35:39,357 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-06 10:45:20,583 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Executing job
2026-01-06 10:45:27,710 | INFO     | py4j.java_gateway | Callback Server Starting
2026-01-06 10:45:27,726 | INFO     | py4j.java_gateway | Socket listening on ('127.0.0.1', 61830)
2026-01-06 10:46:05,965 | INFO     | py4j.clientserver | Python Server ready to receive messages
2026-01-06 10:46:05,966 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-06 10:46:06,648 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 0 - Count: 0
2026-01-06 10:48:03,108 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-06 10:48:03,562 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 1 - Count: 12
2026-01-06 10:48:34,645 | INFO     | py4j.clientserver | Received command c on object id p0
2026-01-06 10:49:02,447 | INFO     | streaming.jobs.votes_streaming_job | [StreamingJob]: Writing Gold Batch ID: 2 - Count: 90
2026-01-06 10:49:03,130 | ERROR    | py4j.clientserver | There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 120, in call
    raise e
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "D:\dev\UCM-BD_DE\votes_manager\streaming\jobs\votes_streaming_job.py", line 231, in _write_to_elasticsearch
    .save()
     ^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1461, in save
    self._jwrite.save()
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\pyspark.zip\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o161.save.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 20.0 failed 1 times, most recent failure: Lost task 1.0 in stage 20.0 (TID 920) (DESKTOP-HMVAKG6.mshome.net executor driver): org.apache.spark.util.TaskCompletionListenerException: Could not write all entries for bulk operation [1/10]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: version_conflict_engine_exception: [32663e79bc34538109ebc377270bf2bccb67f47358b3a5ada7aacc7e7afc5141]: version conflict, required seqNo [12], primary term [1]. current document has seqNo [64] and primary term [1]
	{"update":{"_id":"32663e79bc34538109ebc377270bf2bccb67f47358b3a5ada7aacc7e7afc5141"}}
{"doc_as_upsert":true,"doc":{"id":"ada8d02d-d73b-474e-9f74-170ee0004000","blank_vote":false,"political_party":"Conejo Federal","province_name":"Madrid","province_iso_code":"ES-M","autonomic_region_name":"Comunidad de Madrid","autonomic_region_iso_code":"ES-MD","location":"40.416800,-3.703800","timestamp":1767689250760,"doc_id":"32663e79bc34538109ebc377270bf2bccb67f47358b3a5ada7aacc7e7afc5141"}}

Bailing out...
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:254)
	at org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:185)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
	Suppressed: org.elasticsearch.hadoop.EsHadoopException: Could not write all entries for bulk operation [1/10]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: version_conflict_engine_exception: [32663e79bc34538109ebc377270bf2bccb67f47358b3a5ada7aacc7e7afc5141]: version conflict, required seqNo [12], primary term [1]. current document has seqNo [64] and primary term [1]
	{"update":{"_id":"32663e79bc34538109ebc377270bf2bccb67f47358b3a5ada7aacc7e7afc5141"}}
{"doc_as_upsert":true,"doc":{"id":"ada8d02d-d73b-474e-9f74-170ee0004000","blank_vote":false,"political_party":"Conejo Federal","province_name":"Madrid","province_iso_code":"ES-M","autonomic_region_name":"Comunidad de Madrid","autonomic_region_iso_code":"ES-MD","location":"40.416800,-3.703800","timestamp":1767689250760,"doc_id":"32663e79bc34538109ebc377270bf2bccb67f47358b3a5ada7aacc7e7afc5141"}}

Bailing out...
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.flush(BulkProcessor.java:538)
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.close(BulkProcessor.java:560)
		at org.elasticsearch.hadoop.rest.RestRepository.close(RestRepository.java:226)
		at org.elasticsearch.hadoop.rest.RestService$PartitionWriter.close(RestService.java:122)
		at org.elasticsearch.spark.rdd.EsRDDWriter$$anon$1.onTaskCompletion(EsRDDWriter.scala:74)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)
		... 12 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2898)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2834)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2833)
	at scala.collection.immutable.List.foreach(List.scala:333)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2833)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1253)
	at scala.Option.foreach(Option.scala:437)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3102)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3036)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3025)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:995)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2446)
	at org.elasticsearch.spark.sql.EsSparkSQL$.saveToEs(EsSparkSQL.scala:103)
	at org.elasticsearch.spark.sql.ElasticsearchRelation.insert(DefaultSource.scala:629)
	at org.elasticsearch.spark.sql.DefaultSource.createRelation(DefaultSource.scala:107)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy34.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.spark.util.TaskCompletionListenerException: Could not write all entries for bulk operation [1/10]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: version_conflict_engine_exception: [32663e79bc34538109ebc377270bf2bccb67f47358b3a5ada7aacc7e7afc5141]: version conflict, required seqNo [12], primary term [1]. current document has seqNo [64] and primary term [1]
	{"update":{"_id":"32663e79bc34538109ebc377270bf2bccb67f47358b3a5ada7aacc7e7afc5141"}}
{"doc_as_upsert":true,"doc":{"id":"ada8d02d-d73b-474e-9f74-170ee0004000","blank_vote":false,"political_party":"Conejo Federal","province_name":"Madrid","province_iso_code":"ES-M","autonomic_region_name":"Comunidad de Madrid","autonomic_region_iso_code":"ES-MD","location":"40.416800,-3.703800","timestamp":1767689250760,"doc_id":"32663e79bc34538109ebc377270bf2bccb67f47358b3a5ada7aacc7e7afc5141"}}

Bailing out...
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:254)
	at org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:185)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
	Suppressed: org.elasticsearch.hadoop.EsHadoopException: Could not write all entries for bulk operation [1/10]. Error sample (first [5] error messages):
	org.elasticsearch.hadoop.rest.EsHadoopRemoteException: version_conflict_engine_exception: [32663e79bc34538109ebc377270bf2bccb67f47358b3a5ada7aacc7e7afc5141]: version conflict, required seqNo [12], primary term [1]. current document has seqNo [64] and primary term [1]
	{"update":{"_id":"32663e79bc34538109ebc377270bf2bccb67f47358b3a5ada7aacc7e7afc5141"}}
{"doc_as_upsert":true,"doc":{"id":"ada8d02d-d73b-474e-9f74-170ee0004000","blank_vote":false,"political_party":"Conejo Federal","province_name":"Madrid","province_iso_code":"ES-M","autonomic_region_name":"Comunidad de Madrid","autonomic_region_iso_code":"ES-MD","location":"40.416800,-3.703800","timestamp":1767689250760,"doc_id":"32663e79bc34538109ebc377270bf2bccb67f47358b3a5ada7aacc7e7afc5141"}}

Bailing out...
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.flush(BulkProcessor.java:538)
		at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.close(BulkProcessor.java:560)
		at org.elasticsearch.hadoop.rest.RestRepository.close(RestRepository.java:226)
		at org.elasticsearch.hadoop.rest.RestService$PartitionWriter.close(RestService.java:122)
		at org.elasticsearch.spark.rdd.EsRDDWriter$$anon$1.onTaskCompletion(EsRDDWriter.scala:74)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)
		at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)
		... 12 more

2026-01-06 11:02:54,880 | INFO     | py4j.clientserver | Error while receiving.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-06 11:02:54,886 | INFO     | py4j.clientserver | Error while python server was waiting fora message
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 566, in wait_for_commands
    command = smart_decode(self.stream.readline())[:-1]
                           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto
2026-01-06 11:02:54,887 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-06 11:02:54,887 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-06 11:02:54,888 | ERROR    | root | Exception while sending command.
Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pedro Basso\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\spark\3.5.6\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-01-06 11:02:54,890 | INFO     | py4j.clientserver | Closing down clientserver connection
2026-01-06 11:02:54,891 | INFO     | py4j.clientserver | Closing down clientserver connection
